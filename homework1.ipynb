{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Unnamed: 0   TNFRSF4    CPSF3L    ATAD3C   C1orf86      RER1  \\\n",
      "632   ATCCAGGACGCTAA-1 -0.373290 -0.318454 -0.058893 -0.572970  1.070803   \n",
      "308   ACGGTAACCTTCGC-1 -0.208618 -0.260273 -0.047117 -0.431767 -0.473011   \n",
      "1171  CGATCAGAGGTACT-1  3.298770  1.979825 -0.065353 -0.685601  0.596368   \n",
      "2368  TGAGCTGACTGGAT-1 -0.137419 -0.272108 -0.044461  2.084432  1.851934   \n",
      "1800  GGACCGTGGGAACG-1 -0.276199 -0.351538 -0.056387 -0.640358  0.794003   \n",
      "...                ...       ...       ...       ...       ...       ...   \n",
      "2538  TTCAACACGGACGA-1 -0.260839 -0.281239 -0.051017  1.828419  1.598881   \n",
      "2422  TGCTATACGGTTCA-1 -0.197154 -0.267732 -0.047055 -0.447667 -0.498946   \n",
      "1999  GTGATTCTGTCGAT-1 -0.252105 -0.259212 -0.049145  2.188305 -0.472298   \n",
      "342   ACTACGGAATTTCC-1 -0.251326 -0.266733 -0.049603 -0.449098 -0.499213   \n",
      "2220  TATGTCACGGAACG-1 -0.248963 -0.310563 -0.052375 -0.546919 -0.656221   \n",
      "\n",
      "      TNFRSF25   TNFRSF9  CTNNBIP1       SRM  ...     BACE2      SIK1  \\\n",
      "632  -0.329597 -0.100347  2.850371 -0.542345  ... -0.044789 -0.351657   \n",
      "308  -0.231082 -0.051384 -0.222028 -0.327150  ... -0.124255 -0.181852   \n",
      "1171 -0.392343 -0.118882 -0.436002 -0.676787  ...  0.016349 -0.478486   \n",
      "2368 -0.222094 -0.027675 -0.266254 -0.303533  ... -0.115499 -0.194767   \n",
      "1800 -0.332736 -0.066853 -0.433141 -0.541945  ... -0.013096 -0.408818   \n",
      "...        ...       ...       ...       ...  ...       ...       ...   \n",
      "2538 -0.264609 -0.066739  3.615790 -0.400115  ... -0.096122 -0.241572   \n",
      "2422 -0.234681 -0.047185 -0.242863 -0.333771  ... -0.116372 -0.196894   \n",
      "1999 -0.242175 -0.065442 -0.210244 -0.353150  ... -0.122415 -0.188358   \n",
      "342   3.738395 -0.064675 -0.228979 -0.366764  ... -0.113704 -0.205751   \n",
      "2220 -0.287958 -0.060907 -0.337705 -0.447508  ... -0.062783 -0.307560   \n",
      "\n",
      "      C21orf33    ICOSLG     SUMO3   SLC19A1     S100B     PRMT2  cell_type  \\\n",
      "632  -0.436736 -0.057435 -0.674934 -0.199045 -0.201340 -0.515474          5   \n",
      "308  -0.303258 -0.126010 -0.450171 -0.068619 -0.173019 -0.505753          3   \n",
      "1171 -0.520172 -0.014834 -0.855643 -0.295538 -0.243705 -0.539455          0   \n",
      "2368 -0.288679 -0.133901 -0.486783 -0.077136 -0.207434 -0.531924          1   \n",
      "1800 -0.436615 -0.058229  0.841534 -0.240394 -0.269646 -0.565077          6   \n",
      "...        ...       ...       ...       ...       ...       ...        ...   \n",
      "2538 -0.348520 -0.102784 -0.530547 -0.114394 -0.185173 -0.510901          1   \n",
      "2422 -0.307387 -0.124014 -0.476154 -0.079736 -0.185566 -0.514528          1   \n",
      "1999 -0.319354 -0.117562 -0.450116 -0.074239 -0.159778 -0.494833          0   \n",
      "342  -0.327813 -0.113301 -0.477251 -0.087303 -0.169479 -0.501188          0   \n",
      "2220 -0.377980 -0.088012 -0.635579 -0.163808 -0.225415 -0.537725          2   \n",
      "\n",
      "      cell_type_string  \n",
      "632   FCGR3A Monocytes  \n",
      "308              CD8 T  \n",
      "1171             CD4 T  \n",
      "2368    CD14 Monocytes  \n",
      "1800         Dendritic  \n",
      "...                ...  \n",
      "2538    CD14 Monocytes  \n",
      "2422    CD14 Monocytes  \n",
      "1999             CD4 T  \n",
      "342              CD4 T  \n",
      "2220                 B  \n",
      "\n",
      "[100 rows x 1841 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 读取数据\n",
    "pbmc_data = pd.read_csv('pbmc_data.csv')\n",
    "# 随机展示10行\n",
    "print(pbmc_data.sample(n=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据说明：注：以下的内容主要来自生科院同学+Deepseek-R1+Wiki百科的解释\n",
    "我们需要处理的应该是RNA测序的数据\n",
    "第一列数据表示barcode,代表一个细胞\n",
    "其他列都是基因的名称，数值代表表达量。\n",
    "最后两列是cell type，数值编号和名称。\n",
    "\n",
    "## 研究背景：\n",
    "单细胞测序技术的突破（如10x Genomics、Smart-seq2）使得在单个细胞水平解析基因表达成为可能。传统Bulk RNA测序仅能提供细胞群体的平均表达谱，掩盖了细胞异质性，而单细胞技术揭示了外周血中免疫细胞的多样性。外周血作为免疫系统的主要载体，包含T细胞、B细胞、自然杀伤细胞（NK）、单核细胞等多种免疫亚群，其动态变化与感染、癌症、自身免疫疾病等密切相关。\n",
    "\n",
    "## 外周血研究的核心价值\n",
    "免疫细胞异质性解析：例如，CD8+ T细胞可进一步分为效应T细胞、记忆T细胞和耗竭T细胞，单细胞测序能精准区分这些亚群并揭示其功能状态。\n",
    "\n",
    "疾病机制探索：在癌症中，外周血循环肿瘤细胞（CTCs）和免疫细胞组成可反映肿瘤微环境的状态；在COVID-19中，单细胞数据揭示了重症患者中单核细胞的过度炎症反应。\n",
    "\n",
    "生物标志物发现：通过对比健康人与患者的细胞亚群比例或差异基因，可筛选潜在诊断标志物或治疗靶点（如PD-1在耗竭T细胞中的高表达）。\n",
    "\n",
    "## 医学应用场景\n",
    "肿瘤免疫治疗：分析患者外周血中免疫细胞的功能状态，预测PD-1/PD-L1抑制剂疗效。\n",
    "\n",
    "自身免疫疾病：系统性红斑狼疮（SLE）患者外周血中浆细胞样树突状细胞（pDC）的异常活化提示I型干扰素信号通路的激活。\n",
    "\n",
    "感染性疾病：HIV感染中CD4+ T细胞的耗竭轨迹可通过单细胞轨迹分析（如Monocle）重建。\n",
    "\n",
    "\n",
    "\n",
    "## RNA测序的发展历史：\n",
    "RNA测序技术自20世纪70年代起经历了多代技术革新，其发展历程可分为以下三个阶段：\n",
    "\n",
    "1. 第一代测序技术：Sanger测序（1975-2005）\n",
    "基于链终止法（双脱氧法），通过荧光标记ddNTP终止DNA链延伸，结合毛细管电泳读取序列。其特点是准确性高（误差率<0.1%），但通量低、成本高，主要用于少量RNA转录本的验证，如单个基因的剪接变体分析。\n",
    "\n",
    "2. 第二代测序技术（NGS，2005-2015）\n",
    "以Illumina、Ion Torrent为代表的高通量测序技术，通过边合成边测序（SBS）实现大规模平行测序。NGS的核心优势在于通量高、成本低，能够同时分析全转录组，支持差异基因表达、可变剪接、融合基因等研究。例如，Ion Torrent平台支持靶向转录组测序，可检测超过20,000个基因的表达。\n",
    "\n",
    "3. 第三代测序技术（单分子测序，2015至今）\n",
    "以PacBio和Oxford Nanopore为代表，无需PCR扩增，直接对RNA或cDNA进行长读长测序。Oxford Nanopore的直接RNA测序技术可捕获全长转录本，同时检测RNA修饰（如m6A），解决了短读长技术无法解析复杂剪接异构体和表观修饰的难题。此外，单细胞RNA测序（如10x Genomics、Takara的Shasta系统）实现了单细胞分辨率的转录组分析，揭示细胞异质性。\n",
    "\n",
    "## RNA测序的实现方法\n",
    "RNA测序的流程通常包括以下关键步骤：\n",
    "\n",
    "1. 样本制备与文库构建\n",
    "RNA提取与富集：从组织或细胞中提取总RNA，通过poly-A捕获富集mRNA或保留非编码RNA（如lncRNA、miRNA）。\n",
    "\n",
    "cDNA合成与建库：将RNA反转录为cDNA，添加测序接头和条形码（Barcode）。例如，Takara的Shasta单细胞系统通过微孔芯片实现单细胞分选与文库构建，每个孔可容纳单个细胞并添加特异性条形码，支持高通量单细胞测序。\n",
    "\n",
    "2. 测序技术选择\n",
    "短读长测序（NGS）：适用于基因表达定量和剪接分析，如Illumina平台生成150-300 bp读长数据。\n",
    "\n",
    "长读长测序（三代测序）：如Oxford Nanopore直接测序RNA分子，读长可达数万碱基，用于解析全长转录本和RNA修饰。\n",
    "\n",
    "3. 数据分析\n",
    "比对与定量：使用STAR、Kallisto等工具将测序reads比对到参考基因组，统计基因或转录本表达量（如TPM、FPKM）。\n",
    "\n",
    "差异表达与功能分析：通过DESeq2、edgeR等工具鉴定差异基因，结合GO/KEGG通路富集分析功能。\n",
    "\n",
    "高级应用：单细胞数据分析（如UMAP降维、细胞聚类）、融合基因检测（如STAR-Fusion）等。\n",
    "\n",
    "## RNA测序的具体功能\n",
    "1. 基因表达定量与差异分析\n",
    "RNA-Seq可精确量化基因表达水平，比较不同条件（如疾病vs健康）下的差异表达基因。例如，肿瘤组织中EGFR mRNA的高表达可通过RNA-Seq检测，并用于癌症分型。\n",
    "\n",
    "2. 转录本结构与剪接分析\n",
    "可变剪接：鉴定同一基因的不同剪接异构体，如癌症中常见的异常剪接事件。\n",
    "\n",
    "新转录本发现：长读长测序揭示未注释的转录本，如lncRNA或融合基因（如BCR-ABL1）。\n",
    "\n",
    "3. 表观转录组学研究\n",
    "直接RNA测序技术（如Oxford Nanopore）可检测RNA修饰（如m6A、假尿苷酸），揭示其在基因调控和疾病中的作用。例如，m6A修饰在神经退行性疾病中的异常分布。\n",
    "\n",
    "4. 单细胞分辨率研究\n",
    "单细胞RNA测序（scRNA-seq）解析细胞异质性，如外周血中T细胞亚群（效应T细胞、记忆T细胞）的功能差异，或肿瘤微环境中免疫细胞的动态变化。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据大小:(2638, 1841)\n",
      "列索引如下:\n",
      "Index(['Unnamed: 0', 'TNFRSF4', 'CPSF3L', 'ATAD3C', 'C1orf86', 'RER1',\n",
      "       'TNFRSF25', 'TNFRSF9', 'CTNNBIP1', 'SRM',\n",
      "       ...\n",
      "       'BACE2', 'SIK1', 'C21orf33', 'ICOSLG', 'SUMO3', 'SLC19A1', 'S100B',\n",
      "       'PRMT2', 'cell_type', 'cell_type_string'],\n",
      "      dtype='object', length=1841)\n",
      "细胞类型编码与名称的对应关系:\n",
      "0: CD4 T\n",
      "2: B\n",
      "1: CD14 Monocytes\n",
      "4: NK\n",
      "3: CD8 T\n",
      "5: FCGR3A Monocytes\n",
      "6: Dendritic\n",
      "7: Megakaryocytes\n"
     ]
    }
   ],
   "source": [
    "print(f\"数据大小:{pbmc_data.shape}\")\n",
    "print(f\"列索引如下:\")\n",
    "print(pbmc_data.columns)\n",
    "print(\"细胞类型编码与名称的对应关系:\")\n",
    "cell_types = dict(zip(pbmc_data['cell_type'], pbmc_data['cell_type_string']))\n",
    "for code, name in cell_types.items():\n",
    "    print(f\"{code}: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看了下，有2638个细胞数据\n",
    "\n",
    "每个细胞有1838个基因对应的表达量\n",
    "\n",
    "并且一共有8种细胞，分别对应0~7的数字编码。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#搭建网络框架 这里采取带有batchnorm 和残差连接的 DNN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DeepClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=1838, hidden_dims=[512, 256, 128], output_dim=8):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # 输入层\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.BatchNorm1d(hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # 隐藏层（带残差连接）\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            self.layers.append(\n",
    "                ResidualBlock(hidden_dims[i], hidden_dims[i+1])\n",
    "            )\n",
    "            \n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, out_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(out_dim)\n",
    "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(out_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # 如果输入输出维度不同，添加一个线性映射\n",
    "        self.shortcut = nn.Linear(in_dim, out_dim) if in_dim != out_dim else nn.Identity()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn2(self.fc2(x))\n",
    "        \n",
    "        x += identity\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练器\n",
    "def train_classifier(model, train_loader, val_loader, num_epochs=100):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 使用Adam优化器:0.001学习率，1e-4的L2正则项\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, #学习率调整因子，如果损失不下降就将学习率减半\n",
    "        patience=5, #容忍5个epoch验证损失不下降\n",
    "        verbose=True #打印学习率变化信息\n",
    "    )\n",
    "    \n",
    "    # 损失函数：采用交叉熵\n",
    "    criterion = nn.CrossEntropyLoss()# 注意这里用了交叉熵之后原网络中输出就不需要加softmax层了\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += predicted.eq(batch_y).sum().item()\n",
    "        \n",
    "        val_acc = 100. * correct / total #准确率\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def pre_process(data,batch_size=32):\n",
    "    #分离特征和标签\n",
    "    X = data.iloc[:,1:1839].values\n",
    "    Y = data.iloc[:, 1839].values.astype(np.int64)\n",
    "\n",
    "    #标准化\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X) #标准化为均值0，方差1的数据分布\n",
    "\n",
    "    #划分training_set validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, Y, \n",
    "        test_size=0.2, #验证集占0.2\n",
    "        random_state=42, \n",
    "        stratify=Y\n",
    "    )\n",
    "    # 转换为张量\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    y_train = torch.LongTensor(y_train)\n",
    "    X_val = torch.FloatTensor(X_val)\n",
    "    y_val = torch.LongTensor(y_val)\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\gpu_torch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500:\n",
      "Val Loss: 0.3694, Val Acc: 85.98%\n",
      "Epoch 2/500:\n",
      "Val Loss: 0.3238, Val Acc: 89.96%\n",
      "Epoch 3/500:\n",
      "Val Loss: 0.3220, Val Acc: 92.99%\n",
      "Epoch 4/500:\n",
      "Val Loss: 0.3398, Val Acc: 91.86%\n",
      "Epoch 5/500:\n",
      "Val Loss: 0.3706, Val Acc: 93.18%\n",
      "Epoch 6/500:\n",
      "Val Loss: 0.3760, Val Acc: 93.18%\n",
      "Epoch 7/500:\n",
      "Val Loss: 0.3329, Val Acc: 93.37%\n",
      "Epoch 8/500:\n",
      "Val Loss: 0.3593, Val Acc: 94.13%\n",
      "Epoch 9/500:\n",
      "Val Loss: 0.3991, Val Acc: 93.94%\n",
      "Epoch 10/500:\n",
      "Val Loss: 0.3686, Val Acc: 93.75%\n",
      "Epoch 11/500:\n",
      "Val Loss: 0.3606, Val Acc: 93.94%\n",
      "Epoch 12/500:\n",
      "Val Loss: 0.3966, Val Acc: 93.75%\n",
      "Epoch 13/500:\n",
      "Val Loss: 0.3951, Val Acc: 93.37%\n",
      "Epoch 14/500:\n",
      "Val Loss: 0.3823, Val Acc: 94.51%\n",
      "Epoch 15/500:\n",
      "Val Loss: 0.3903, Val Acc: 93.94%\n",
      "Epoch 16/500:\n",
      "Val Loss: 0.3966, Val Acc: 94.51%\n",
      "Epoch 17/500:\n",
      "Val Loss: 0.4235, Val Acc: 93.94%\n",
      "Epoch 18/500:\n",
      "Val Loss: 0.3944, Val Acc: 94.70%\n",
      "Epoch 19/500:\n",
      "Val Loss: 0.4232, Val Acc: 93.75%\n",
      "Epoch 20/500:\n",
      "Val Loss: 0.3869, Val Acc: 94.32%\n",
      "Epoch 21/500:\n",
      "Val Loss: 0.4112, Val Acc: 93.94%\n",
      "Epoch 22/500:\n",
      "Val Loss: 0.3990, Val Acc: 94.32%\n",
      "Epoch 23/500:\n",
      "Val Loss: 0.4120, Val Acc: 93.18%\n",
      "Epoch 24/500:\n",
      "Val Loss: 0.4061, Val Acc: 94.51%\n",
      "Epoch 25/500:\n",
      "Val Loss: 0.3942, Val Acc: 94.51%\n",
      "Epoch 26/500:\n",
      "Val Loss: 0.4267, Val Acc: 93.94%\n",
      "Epoch 27/500:\n",
      "Val Loss: 0.4241, Val Acc: 94.13%\n",
      "Epoch 28/500:\n",
      "Val Loss: 0.4007, Val Acc: 94.13%\n",
      "Epoch 29/500:\n",
      "Val Loss: 0.4066, Val Acc: 93.94%\n",
      "Epoch 30/500:\n",
      "Val Loss: 0.4098, Val Acc: 94.13%\n",
      "Epoch 31/500:\n",
      "Val Loss: 0.4157, Val Acc: 93.94%\n",
      "Epoch 32/500:\n",
      "Val Loss: 0.3949, Val Acc: 94.32%\n",
      "Epoch 33/500:\n",
      "Val Loss: 0.3950, Val Acc: 94.70%\n",
      "Epoch 34/500:\n",
      "Val Loss: 0.4081, Val Acc: 94.32%\n",
      "Epoch 35/500:\n",
      "Val Loss: 0.4057, Val Acc: 94.13%\n",
      "Epoch 36/500:\n",
      "Val Loss: 0.4024, Val Acc: 94.32%\n",
      "Epoch 37/500:\n",
      "Val Loss: 0.3994, Val Acc: 94.70%\n",
      "Epoch 38/500:\n",
      "Val Loss: 0.3903, Val Acc: 94.32%\n",
      "Epoch 39/500:\n",
      "Val Loss: 0.4035, Val Acc: 94.13%\n",
      "Epoch 40/500:\n",
      "Val Loss: 0.3893, Val Acc: 94.32%\n",
      "Epoch 41/500:\n",
      "Val Loss: 0.3854, Val Acc: 94.51%\n",
      "Epoch 42/500:\n",
      "Val Loss: 0.3772, Val Acc: 94.51%\n",
      "Epoch 43/500:\n",
      "Val Loss: 0.3820, Val Acc: 94.70%\n",
      "Epoch 44/500:\n",
      "Val Loss: 0.3908, Val Acc: 94.51%\n",
      "Epoch 45/500:\n",
      "Val Loss: 0.3886, Val Acc: 94.51%\n",
      "Epoch 46/500:\n",
      "Val Loss: 0.3845, Val Acc: 94.51%\n",
      "Epoch 47/500:\n",
      "Val Loss: 0.4123, Val Acc: 94.13%\n",
      "Epoch 48/500:\n",
      "Val Loss: 0.3925, Val Acc: 94.51%\n",
      "Epoch 49/500:\n",
      "Val Loss: 0.4009, Val Acc: 94.51%\n",
      "Epoch 50/500:\n",
      "Val Loss: 0.4036, Val Acc: 94.32%\n",
      "Epoch 51/500:\n",
      "Val Loss: 0.4147, Val Acc: 94.32%\n",
      "Epoch 52/500:\n",
      "Val Loss: 0.3891, Val Acc: 94.70%\n",
      "Epoch 53/500:\n",
      "Val Loss: 0.4130, Val Acc: 93.94%\n",
      "Epoch 54/500:\n",
      "Val Loss: 0.4008, Val Acc: 94.51%\n",
      "Epoch 55/500:\n",
      "Val Loss: 0.3881, Val Acc: 94.51%\n",
      "Epoch 56/500:\n",
      "Val Loss: 0.4023, Val Acc: 94.51%\n",
      "Epoch 57/500:\n",
      "Val Loss: 0.3821, Val Acc: 94.51%\n",
      "Epoch 58/500:\n",
      "Val Loss: 0.3925, Val Acc: 94.51%\n",
      "Epoch 59/500:\n",
      "Val Loss: 0.4045, Val Acc: 94.89%\n",
      "Epoch 60/500:\n",
      "Val Loss: 0.3895, Val Acc: 94.51%\n",
      "Epoch 61/500:\n",
      "Val Loss: 0.4079, Val Acc: 94.51%\n",
      "Epoch 62/500:\n",
      "Val Loss: 0.3744, Val Acc: 94.32%\n",
      "Epoch 63/500:\n",
      "Val Loss: 0.3997, Val Acc: 94.70%\n",
      "Epoch 64/500:\n",
      "Val Loss: 0.4184, Val Acc: 93.94%\n",
      "Epoch 65/500:\n",
      "Val Loss: 0.4042, Val Acc: 94.13%\n",
      "Epoch 66/500:\n",
      "Val Loss: 0.3992, Val Acc: 94.70%\n",
      "Epoch 67/500:\n",
      "Val Loss: 0.3861, Val Acc: 94.70%\n",
      "Epoch 68/500:\n",
      "Val Loss: 0.4041, Val Acc: 94.51%\n",
      "Epoch 69/500:\n",
      "Val Loss: 0.4090, Val Acc: 94.32%\n",
      "Epoch 70/500:\n",
      "Val Loss: 0.3988, Val Acc: 94.51%\n",
      "Epoch 71/500:\n",
      "Val Loss: 0.4195, Val Acc: 93.94%\n",
      "Epoch 72/500:\n",
      "Val Loss: 0.4016, Val Acc: 94.89%\n",
      "Epoch 73/500:\n",
      "Val Loss: 0.3927, Val Acc: 94.51%\n",
      "Epoch 74/500:\n",
      "Val Loss: 0.4066, Val Acc: 94.51%\n",
      "Epoch 75/500:\n",
      "Val Loss: 0.3960, Val Acc: 94.89%\n",
      "Epoch 76/500:\n",
      "Val Loss: 0.3976, Val Acc: 94.32%\n",
      "Epoch 77/500:\n",
      "Val Loss: 0.3898, Val Acc: 94.70%\n",
      "Epoch 78/500:\n",
      "Val Loss: 0.3930, Val Acc: 94.51%\n",
      "Epoch 79/500:\n",
      "Val Loss: 0.4133, Val Acc: 94.32%\n",
      "Epoch 80/500:\n",
      "Val Loss: 0.3941, Val Acc: 94.70%\n",
      "Epoch 81/500:\n",
      "Val Loss: 0.4022, Val Acc: 94.13%\n",
      "Epoch 82/500:\n",
      "Val Loss: 0.3867, Val Acc: 94.70%\n",
      "Epoch 83/500:\n",
      "Val Loss: 0.3879, Val Acc: 94.51%\n",
      "Epoch 84/500:\n",
      "Val Loss: 0.4001, Val Acc: 94.32%\n",
      "Epoch 85/500:\n",
      "Val Loss: 0.3984, Val Acc: 94.89%\n",
      "Epoch 86/500:\n",
      "Val Loss: 0.4044, Val Acc: 94.70%\n",
      "Epoch 87/500:\n",
      "Val Loss: 0.3905, Val Acc: 94.70%\n",
      "Epoch 88/500:\n",
      "Val Loss: 0.4108, Val Acc: 94.32%\n",
      "Epoch 89/500:\n",
      "Val Loss: 0.4220, Val Acc: 94.13%\n",
      "Epoch 90/500:\n",
      "Val Loss: 0.3966, Val Acc: 94.32%\n",
      "Epoch 91/500:\n",
      "Val Loss: 0.3980, Val Acc: 94.70%\n",
      "Epoch 92/500:\n",
      "Val Loss: 0.4271, Val Acc: 94.13%\n",
      "Epoch 93/500:\n",
      "Val Loss: 0.3960, Val Acc: 94.51%\n",
      "Epoch 94/500:\n",
      "Val Loss: 0.4108, Val Acc: 94.32%\n",
      "Epoch 95/500:\n",
      "Val Loss: 0.4017, Val Acc: 94.51%\n",
      "Epoch 96/500:\n",
      "Val Loss: 0.4107, Val Acc: 94.51%\n",
      "Epoch 97/500:\n",
      "Val Loss: 0.3894, Val Acc: 94.70%\n",
      "Epoch 98/500:\n",
      "Val Loss: 0.4276, Val Acc: 94.32%\n",
      "Epoch 99/500:\n",
      "Val Loss: 0.3889, Val Acc: 94.51%\n",
      "Epoch 100/500:\n",
      "Val Loss: 0.3977, Val Acc: 93.94%\n",
      "Epoch 101/500:\n",
      "Val Loss: 0.3951, Val Acc: 94.13%\n",
      "Epoch 102/500:\n",
      "Val Loss: 0.4105, Val Acc: 94.13%\n",
      "Epoch 103/500:\n",
      "Val Loss: 0.3973, Val Acc: 94.32%\n",
      "Epoch 104/500:\n",
      "Val Loss: 0.4101, Val Acc: 94.32%\n",
      "Epoch 105/500:\n",
      "Val Loss: 0.3968, Val Acc: 94.32%\n",
      "Epoch 106/500:\n",
      "Val Loss: 0.4059, Val Acc: 94.13%\n",
      "Epoch 107/500:\n",
      "Val Loss: 0.4005, Val Acc: 94.70%\n",
      "Epoch 108/500:\n",
      "Val Loss: 0.3787, Val Acc: 94.51%\n"
     ]
    }
   ],
   "source": [
    "#首次训练 baseline\n",
    "train_loader, val_loader = pre_process(pbmc_data,batch_size=32)\n",
    "model = DeepClassifier(\n",
    "    input_dim=1838,\n",
    "    hidden_dims=[512,256,128,64],\n",
    "    output_dim=8\n",
    ")\n",
    "# 2. 记录开始时间\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# 3. 训练模型\n",
    "print(\"开始训练...\")\n",
    "# 2. 调用训练器进行训练\n",
    "train_classifier(\n",
    "    model=model,\n",
    "    train_loader=train_loader,  # 之前预处理得到的训练数据加载器\n",
    "    val_loader=val_loader,      # 之前预处理得到的验证数据加载器\n",
    "    num_epochs=500              # 训练轮数\n",
    ")\n",
    "# 4. 打印训练时间\n",
    "training_time = time.time() - start_time\n",
    "print(f'\\n训练完成')\n",
    "print(f'总训练时间: {training_time:.2f} 秒')\n",
    "\n",
    "# 5. 在验证集上进行最终评估\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in val_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_X)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += batch_y.size(0)\n",
    "        correct += predicted.eq(batch_y).sum().item()\n",
    "\n",
    "final_acc = 100. * correct / total\n",
    "print(f'最终验证集准确率: {final_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
